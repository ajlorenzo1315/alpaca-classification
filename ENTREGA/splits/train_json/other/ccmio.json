{
    "TÃ­tulo": "Fine-Grain Parallel CM RIVL",
    "Cuerpo": "MIME-Version: 1.0 Server: CERN/3.0 Date: Sunday, 01-Dec-96 18:54:06 GMT Content-Type: text/html Content-Length: 28767 Last-Modified: Monday, 06-May-96 23:26:29 GMT Fine-Grain Parallel CM RIVL Fine-Grain Parallel CM RIVL:  A Step Towards Real-Time Multimedia Processing Jonathan Barber ( barber@cs.cornell.edu ) Sugata Mukhopadhyay ( sugata@cs.cornell.edu ) CS516 Final Project Professor Thorsten von Eicken Department of Computer Science Cornell University 0.0 Table of Contents 1.0 Abstract 2.0 Introduction 3.0 RIVL and the Generic Parallel Paradigm 3.1 The RIVL Graph 3.2 Parallelizing RIVL 3.3 Continuous Media Parallel RIVL 4.0 Implementations 4.1 Shared Memory Implementation 4.2 Networked Implementation 4.3 Implementation Caveats 5.0 Performance Results 6.0 Extensions and Robustness 7.0 Conclusions 8.0 References Go Back 1.0  Abstract Any form of multimedia processing is typically computationally expensive. In Section 3.0, we describe a generic method for parallelizing most of the image operations in RIVL, by exploiting the way that RIVL processes an inputted set of images. In Section 4.0, we describe two implementations of Parallel CM RIVL (PRIVL). Go Back 3.0  RIVL and the Generic Parallel Paradigm Go Back 3.1  The RIVL Graph We begin our discussion of RIVL by introducing the RIVL Evaluation Graph. The multimedia data is then processed on the second traversal, which conforms to a left-to-right traversal of the RIVL graph, propagating the input data forwards through the graph, only operating on data that is relevant to the final output image. Combining this notion with the fact that most of the image processing operations in RIVL do not create dependencies from one pixel to another in a given input image, we can derive a simple for mechanism for \"dividing up the work\", and parallelizing RIVL. According the figure above, the amount of data fetched from each read node is no longer a function of the output of the write node, but is now a function of: the process's Logical ID# the total number of processes and, is a function of the write node's output That is, each RIVL process  is responsible for computing a different, independent portion of the final output data, which is based on the above parameters. With CM RIVL, there is an initial setup phase for each slave process and the master process, as previously described  (the Master process sends each slave its logical ID#, the total number of processes, and a copy of the RIVL script. When the CMO has captured all of its input data for a single output image, it contacts the master's Parallel Synchronization Device, and tells each RIVL process (slaves and the Master) that data is ready to be fetched, and that computation can begin ASAP. Each RIVL process then fetches only the input data it needs to generate its segment of the output data, and makes the left-to-right traversal through the graph. Go Back 4.1  Shared Memory Implementation The shared-memory implementation is illustrated above. Once the message is received by each RIVL process, a handler is invoked which tells the RIVL process that it can begin evaluating its RIVL graph on the transferred data. The process synchronization mechanism is implicit with the actual data-transfer, in that, a RIVL process cannot begin evaluating its graph on a given frame segment, until it receives an Active-message from the Master process. When the Master process received active-messages from any slave process, the slave process attempted to invoke an AM handler in the Master that existed in the slave, but not in the handler. The situation was the same when a slave process received an Active Message from the Master. The logical ID# corresponds to a process handler's virtual address, which is then invoked from Active-Messages. Modifying the Networked implementation should prove more trouble-some, and while improving the overall load-balance, will probably increase the communication overhead, as more Active-Message will have to be sent and processed. In both our shared-memory implementation and our networked implementation, we obtained good speedups up to four processors. We do not have results for more than four processors. However, by examining our results, we can determine that under the current implementations, the processes running Parallel CM RIVL will not be load-balanced.",
    "ground_truth": "other"
}
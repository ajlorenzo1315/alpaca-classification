{
    "TÃ­tulo": "\nQuery By Humming -- Musical Information Retrieval in an Audio Database\n",
    "Cuerpo": "MIME-Version: 1.0 Server: CERN/3.0 Date: Sunday, 24-Nov-96 21:32:38 GMT Content-Type: text/html Content-Length: 27263 Last-Modified: Wednesday, 27-Sep-95 23:06:43 GMT Query By Humming -- Musical Information Retrieval in an Audio Database ACM Multimedia 95 - Electronic Proceedings November 5-9, 1995 San Francisco, California Query By Humming -- Musical Information Retrieval in an Audio Database Asif Ghias Department of Computer Science 4130 Upson Hall Cornell University Ithaca, NY 14853 US ghias@cs.cornell.edu Jonathan Logan Department of Computer Science 4130 Upson Hall Cornell University Ithaca, NY 14853 US logan@ghs.com David Chamberlin School of Electrical Engineering 224 Phillips Hall Cornell University Ithaca, NY 14853 US chamberlin@engr.sgi.com Brian C. Smith Department of Computer Science 4130 Upson Hall Cornell University Ithaca, NY 14853 US bsmith@cs.cornell.edu ACM Copyright Notice Abstract The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Table of Contents Introduction System Architecture Tracking Pitch in Hummed Queries Tracking pitch Searching the database Evaluation Robustness Performance Future directions and Related Work References Introduction Next generation databases will include image, audio and video data in addition to traditional text and numerical data. Similarly a natural way of querying an audio database (of songs) is to hum the tune of a song. In this paper, we address the issue of how to specify a hummed query and report on an efficient query execution implementation using approximate pattern matching. <-- Table of Contents System Architecture There are three main components to the our system: a pitch-tracking module, a melody database, and a query engine. <-- Table of Contents Tracking Pitch in Hummed Queries This section describes how user input to the system (humming) is converted into a sequence of relative pitch transitions. The pitch of the complex tone, however, was the same as that prior to the elimination of the fundamental [12] Since we were interested in tracking pitch in humming, we examined methods for automatically tracking pitch in a human voice. Hess [5] describes a model of the vocal cords as proposed by Hirano [6] . Therefore, if we want to model a speech signal, we start with a train of excitation pulses as shown in figure 2. Therefore, tracking the frequency of this peaks should give us the pitch of the signal. For an explanation, the reader is directed to Oppenheim and Schafer's original work in [10] or in a more compact form in [11] . <-- Tracking Pitch in Hummed Queries <-- Table of Contents Searching the database Having described how the user input (a hummed tune) is converted into a string in a 3 letter alphabet, we now discuss our method for searching an audio database. Our method of searching the database is simple. Figure 5 summarizes the various forms of  errors anticipated in a typical pattern matching scheme. Three forms of anticipated errors with one mismatch The algorithm that we adopted for this purpose is described by Baesa-Yates and Perleberg [1] . Each of the errors in the figure above corresponds to k = 1 . This design gives the user an opportunity to perform queries even if the user is not sure of some notes within the tune. Robustness The effectiveness of this method is directly related to the accuracy with which pitches that are hummed can be tracked and the accuracy of the melodic information within the database. IEEE Transactions on Acoustics, Speech and Signal Processing , ASSP-24(1):2-8, Feb 1976.",
    "ground_truth": "other"
}
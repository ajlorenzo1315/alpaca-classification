{
    "TÃ­tulo": "Transpose Case",
    "Cuerpo": "I checked for malloc failures but that wasn't it(*). So my results here are very sketchy. (*)Well, there is no problem on eureka in this respect. Although it is not what we are aiming for, a 4x4 grid on eureka achieves 35.xx MFLOPS when the local size hits 400x400 or so. (**)The problem has been fixed -- however, it was a matter of using Irecv instead of receive (basically) in implementing the collect (perhaps I should have just used MPI_Allgather). However, these messages were not very big -- I would need to read about the SP2 architecture I guess but this seems pretty fragile to me. Basically, add 2 MFLOPS to everything here. The code enclosed does not perform accuracy testing -- it was removed to make the runs because it creates the global matrix on each processor. We do have a version (on both spice and eureka) that does the testing. main.c csmmult1.c colrow1.c globals.h rand.c Note : There are at least 4 simple things we could do to improve the performance of this code. That is send the blocks immediately to the processor that they will arrive at after both the scatter and the permute step. To really make the code unreadable, we believe that you could use non-blocking sends to overlap the copying to the send buffer with the sending of blocks. (1 hour to recode and test) 2. Using MPI_Allgather instead of our hand-written bucket-collect. There is a simple test to see if you are sending to yourself for all of these routines. This might improve performance a great deal on grids that are far from sqaure (although, this does partially void #2 -- or perhaps not, if MPI is \"smart\" enough to figure this out itself). The only \"drawback\" would be that it really wouldn't be implementing the same code on the 1x1 mesh and on a small machine like eureka this MIGHT make the scalability appear either \"bad\" or hard to figure an alpha, beta, gamma equation for. On a square mesh there is a very simple trick that would REALLY speed things up (I think). Then you do the analogous thing for the columns of B.  I am pretty sure that Prof. van de Geijn discussed this in class as one of the shortcuts. I really mention it just to point out that that is NOT what we did (because we are presenting data for the square mesh case).",
    "ground_truth": "other"
}
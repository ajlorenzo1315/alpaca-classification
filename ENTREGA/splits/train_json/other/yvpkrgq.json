{
    "Título": "Sin título",
    "Cuerpo": "by Rich Sutton University of Massachusetts rich@cs.umass.edu Presented at the AAAI Stanford Spring Symposium on Machine Learning and Information Access March 26, 1996 with many thanks to Rik Belew and Jude Shavlik Introductory Patter In this talk we will try to take a new look at the learning problem in information access. I am a newcomer to information access, but I have experience in reinforcement learning, and one of the main lessons of reinforcement learning is that it is really important to understand the true nature of the learning problem you want to solve. The same may be true for information access. Right now we use training sets of documents labeled by experts as relevant or not relevant. How much better it would be if we could generate some kind of training data online, from the normal use of the system. The data may be imperfect and unclear, but certainly it will be plentiful! Its training data was more real. This then is the challenge: to think about information access and uncover the real structure of the learning problem. How can learning be done online? Learning thrives on data, data, data! The second examines how parts of the learning problem in information access are like those solved by reinforcement learning methods. In the third part of the talk we examine some of this special structure and what kind of new learning methods might be applied to it. Conclusions (in advance) Learning in IA (Information Access) is like learning everywhere you are never told the right answers its a sequential problem - actions affect opportunities Reinforcement Learning addresses these issues Learning can be powerful when done online (from normal operation) What is online data/feedback like in IA? Reinforcement Learning Learning by trial and error, rewards and punishments, Active, multidisciplinary research area An overall approach to AI based on learning from interaction with the environment integrates learning, planning, reacting... handles stochastic, uncertain environments Recent large-scale, world-class applications Not about particular learning mechanisms Is about learning with less helpful feedback Classical Machine Learning - Supervised Learning situation1  --->  action1     then correct-action1 situation2  --->  action2     then correct-action2 . agent never told which action is correct agent told nothing about actions not selected actions may affect next situation object is to maximize all future rewards It's not just a harder problem, it's a real problem Problems with relevance feedback: what about all the documents not shown? prediction) can't learn from normal operation Applications of RL TD-Gammon and Jellyfish -- Tesauro, Dahl Elevator control -- Crites Job-shop scheduling -- Zhang & Dietterich Mobile robot controllers -- Lin, Miller, Thrun, ... Computer Vision -- Peng et al. Natural language / dialog tuning -- Gorin, Henis Characters for interactive games -- Handelman & Lane Airline seat allocation -- Hutchinson Manufacturing of Composite materials -- Sofge & White Key Ideas of RL Algorithms Value Functions Like a heuristic state evaluation function -- but learned Approximates the expected future reward after a state or action The idea:\tlearn \"how good\" an action is, rather than whether or not it is the best, taking into account long-term affects Value functions vastly simplify everything TD Methods An efficient way of learning to predict (e.g., value functions) from experience and search Learning a guess from a guess A Large Space of RL Algorithms Major Components of an RL Agent Policy - what to do Reward - what is good Value - what is good because it predicts reward Model - what follows what Info-Access Applications of RL Anytime you have decisions to be made and desired choice is not immediately clear Anytime you want to make long-term predictions Classical IR Querying/Routing/Filtering as RL Situation = Query or user model + Documents Actions\t  = Present document? The Multi-Step, Sequential Nature of IA the web page that led to the web page the request of user that enabled a much better query the query whose results enabled user to refine his next query the ordering of search steps the document that turned out NOT to be useful the series of searches, each building on the prior's results Imagine an Ideal Info-Access System Continuous oportunity to provide query info: keywords, type specs, feedback Continuously updated list of proposed documents find the good ones as soon as possible! Shortcutting Feedback is often more than good/bad Often it does indicate the desired response not for the one situation, but for the whole sequence of prior situations Each good document is a positive example    -  this is what I was looking for a negative example  -  why wasn't this found earlier? The classical context Large numbers of documents (e.g., 2 million) a few queries (e.g., 200) No way the queries can be used to learn about the docs The Web Large numbers of documents Even more queries There will always be more readings than writings Thus, we can learn about the docs How good are they? Recommendations: reviewed journals movie critics cool site of the day # visitors to site what your colleagues are talking about \"Its hard to find the good stuff on the web\" But in classical IR there is no concept of good stuff docs are relevant or not, but not good or bad Differences and Similarities between Users Now users provide feedback as a favor, to help others, or because they are paid or the program forces them to They ought to be providing feedback for selfish reasons Suppose you had a personal research assistant... wouldn't you tell him what you liked and didn't like?",
    "ground_truth": "other"
}
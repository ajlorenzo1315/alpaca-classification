{
    "TÃ­tulo": "Keshav Pingali\n",
    "Cuerpo": "MIME-Version: 1.0 Server: CERN/3.0 Date: Wednesday, 20-Nov-96 18:57:29 GMT Content-Type: text/html Content-Length: 5701 Last-Modified: Thursday, 30-Nov-95 21:21:15 GMT Keshav Pingali Keshav Pingali Associate Professor PhD MIT, 1986 My research group works in the areas of programming languages and compilers for parallel architectures. Our goal is to develop tools for generating parallel code for applications programs that deal with large sparse matrices. The techniques used almost always produce a system of algebraic equations that involve large sparse matrices. This enables us to use tools from the restructuring compiler area. We will extend our approach to direct methods for solving linear systems and to applications that require adaptive mesh refinement. This project builds on our earlier work on restructuring compilation techniques for dense matrix programs. To get good performance, the compiler must not only parallelize but must also ensure locality of reference by matching code and data distribution; when non-local references must be made, block transfers are preferable to many small messages. We recently developed the best algorithm known for the automatic alignment of computation and data and are incorporating it into our compiler test-bed. In earlier work, we developed a novel loop restructuring technique called access normalization, which transforms loop nests for increased locality and potential for block transfers, and implemented it in the LAMBDA loop transformation toolkit - our paper summarizing these results won the best paper prize at ASPLOS V.  We worked with Hewlett-Packard to transfer this technology to HP's FORTRAN compiler product line for uniprocessors and multiprocessors. We have developed new frameworks for program analysis and optimization based on the dependence flow graph (DFG). The DFG knits together the data and control dependence information of a program, permitting the development of optimization algorithms that generate better code than is possible with competing approaches. Our results are of independent interest; for example, we recently developed optimal algorithms for control dependence problems, answering a foundational question that had been open for almost a decade. Professional Activities Panel member and organizer, ACM Symposium on Principles and Practice of Parallel Programming, 1995 Member, NSF National Young Investigator (NYI) Awards Panel Consultant: Hewlett Packard Labs, Intel Corporation, Army Ballistic Research Labs, Odyssey Research, Math Sciences Institute Referee/Reviewer: ACM TOPLAS, IEEE Transactions on Computers, Journal of Parallel and Distributed Computing, Journal of Supercomputing, IEEE Computer Editorial Board, International Journal of Parallel Programming Awards National Science Foundation Presidential Young Investigator (1989-1994) IBM Faculty Development Award (1986-88) Best paper prize,  ASPLOS V, 1992 Lectures Fast algorithms for control dependence problems. Hewlett-Packard Corporation, Chelmsford,  Massachusetts, January 1995. Computer Science Department, Wayne State University, Detroit, Michigan, February 1995. Microsoft Research Laboratories, Redmond, Washington, June 1995. Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computers (LCPC), Lecture Notes in Computer Science 892, Ithaca, NY (August 1994) 46-60  (with David Bau, Induprakas Kodukula, Vladimir Kotlyar,  and Paul Stodghill). ACM SIGPLAN '95  Conference on Programming Language Design and Implementation (PLDI June 1995), 171-185 (with Gianfranco Bilardi). Return to: 1994-1995 Annual Report Home Page Departmental Home Page If you have questions or comments please contact: www@cs.cornell.edu. Last modified: 24 November 1995 by Denise Moore (denise@cs.cornell.edu).",
    "ground_truth": "faculty"
}
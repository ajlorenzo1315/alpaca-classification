{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48dcfc3-8f75-459d-bfcc-169cd234af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from keybert import KeyBERT\n",
    "import json\n",
    "import tqdm\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dd2bb5-bb1c-4cf3-8dc4-b9b53f0ee351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta de la carpeta que contiene los archivos origen\n",
    "folder_path = \"../data/splits/train/\"  \n",
    "\n",
    "# Lista creada para almacenar el texto del cuerpo de todas las páginas\n",
    "all_page_texts = []\n",
    "\n",
    "# Function defined to extract the title and summary from the page.\n",
    "def extract_title_and_summary(file_path,dirs, limit_link=3,sentences_count=1,long_text=1024):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Analyze the HTML content using BeautifulSoup.\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract the page title\n",
    "    title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all('a'):\n",
    "        if 'href' in a.attrs:\n",
    "            if len(links) < limit_link:\n",
    "                links.append(a['href'])\n",
    "            else:\n",
    "                break\n",
    "        #print(\"Enlaces en la página:\", links)\n",
    "\n",
    "    # Extract the text from the page's body\n",
    "    page_text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "    # Perform summarization using sumy\n",
    "    parser = PlaintextParser.from_string(page_text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)  # You can adjust the number of sentences in the summary.\n",
    "    #print(summary)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "\n",
    "    return {\"Title\": title, \"Body\": summary_text, \"link\": links ,\"ground_truth\": dirs,\"1024_text\":text[:long_text]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1877a8a0-7493-414f-9c46-4fe378304524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo datos: 100%|██████████████████████| 109/109 [00:00<00:00, 183.86it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 403/403 [00:03<00:00, 124.27it/s]\n",
      "Extrayendo datos:  10%|██▏                   | 302/3011 [00:10<01:04, 42.20it/s]<ipython-input-9-b4d5ecffc733>:13: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n",
      "Extrayendo datos: 100%|█████████████████████| 3011/3011 [01:39<00:00, 30.31it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 899/899 [00:07<00:00, 115.88it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 145/145 [00:00<00:00, 222.90it/s]\n",
      "Extrayendo datos: 100%|████████████████████| 1312/1312 [00:06<00:00, 203.71it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 744/744 [00:09<00:00, 81.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"../data/splits/train\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"../data/splits/train_1024/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Process all files in the folder\n",
    "for dirs in os.listdir(folder_path):\n",
    "    for filename in tqdm.tqdm(os.listdir(os.path.join(folder_path, dirs)), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, dirs, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_title_and_summary(file_path,dirs)\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder,dirs)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a29f3-1975-4885-84e0-c81448d15722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffddb7-d60b-468f-b783-5aae13a99c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

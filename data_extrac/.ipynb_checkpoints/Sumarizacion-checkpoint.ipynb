{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a8df31-3830-47d3-a5c3-a6acdc2a0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (3.8.1)\n",
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from sumy) (2.31.0)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
      "  Downloading lxml-4.9.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from pycountry>=18.2.23->sumy) (68.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (2023.7.22)\n",
      "Downloading lxml-4.9.3-cp311-cp311-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=ec8a026e52141a5f69e8adc17dce46626b4af8ebcd95fa0d6e0cfcb10bcdc6f1\n",
      "  Stored in directory: /home/alourido/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=c913125f8d23c52c1a7c5995e0adb86feff94f87b0174c179368dc50840908a8\n",
      "  Stored in directory: /home/alourido/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=aa15f3fd379717de9ca8a88b6761c56bf234a3ca2fc6f6100ae881264ff39e61\n",
      "  Stored in directory: /home/alourido/.cache/pip/wheels/cd/29/8b/617685ed7942656b36efb06ff9247dbe832e3f4f7724fffc09\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, lxml, chardet, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 lxml-4.9.3 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4b340f-e6d1-4e7a-a403-d0e4cb91d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alourido/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import nltk\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "nltk.download('punkt')\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "898f6776-f9ed-47a1-949b-54e07ab1a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo datos: 100%|██████████████████████| 109/109 [00:00<00:00, 200.69it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 403/403 [00:03<00:00, 120.66it/s]\n",
      "Extrayendo datos:  10%|██▏                   | 307/3011 [00:11<01:04, 42.13it/s]/tmp/ipykernel_7618/1659780632.py:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n",
      "Extrayendo datos: 100%|█████████████████████| 3011/3011 [01:44<00:00, 28.85it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 899/899 [00:07<00:00, 114.43it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 145/145 [00:00<00:00, 219.83it/s]\n",
      "Extrayendo datos: 100%|████████████████████| 1312/1312 [00:06<00:00, 200.33it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 744/744 [00:09<00:00, 79.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"./data/splits/train\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"./data/splits/train_new/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function defined to extract the title and summary from the page.\n",
    "def extract_title_and_summary(file_path,dirs, limit_link=3,sentences_count=1):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Analyze the HTML content using BeautifulSoup.\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract the page title\n",
    "    title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all('a'):\n",
    "        if 'href' in a.attrs:\n",
    "            if len(links) < limit_link:\n",
    "                links.append(a['href'])\n",
    "            else:\n",
    "                break\n",
    "        #print(\"Enlaces en la página:\", links)\n",
    "\n",
    "    # Extract the text from the page's body\n",
    "    page_text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "    # Perform summarization using sumy\n",
    "    parser = PlaintextParser.from_string(page_text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)  # You can adjust the number of sentences in the summary.\n",
    "    #print(summary)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "\n",
    "    return {\"Title\": title, \"Body\": summary_text, \"link\": links ,\"ground_truth\": dirs}\n",
    "\n",
    "# Process all files in the folder\n",
    "for dirs in os.listdir(folder_path):\n",
    "    for filename in tqdm.tqdm(os.listdir(os.path.join(folder_path, dirs)), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, dirs, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_title_and_summary(file_path,dirs)\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder,dirs)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e379e012-4bf2-4e82-b159-8d14b126aa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo datos: 100%|█████████████████████| 1659/1659 [01:16<00:00, 21.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"./data/splits/test\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"./data/splits/test_new/test\"\n",
    "# Process all files in the folder\n",
    "for filename in tqdm.tqdm(os.listdir(folder_path), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_title_and_summary(file_path,'unknow')\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70af9694-fe40-48fd-8512-a1b60916f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Palabras clave a buscar en el campo \"Resultado\"\n",
    "palabras_clave = ['course', 'department', 'faculty', 'other', 'project', 'staff', 'student']\n",
    "\n",
    "# Cargar el JSON desde un archivo\n",
    "with open('resultados_2023-11-06_03-09-05.json', 'r', encoding='utf-8') as archivo:\n",
    "    datos = json.load(archivo)\n",
    "\n",
    "# Limpiar el campo \"Resultado\"\n",
    "for elemento in datos:\n",
    "    # Encontrar la primera palabra clave en \"Resultado\"\n",
    "    resultado = next((palabra for palabra in palabras_clave if palabra in elemento[\"Resultado\"]), \"NONE\")\n",
    "    elemento[\"Resultado\"] = resultado\n",
    "\n",
    "# Escribir el JSON limpio a un nuevo archivo\n",
    "with open('datos_limpios.json', 'w', encoding='utf-8') as archivo:\n",
    "    json.dump(datos, archivo, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c2241f-b9ce-479d-8266-4b22aa142396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aagelci.json', 'abdjgiz.json', 'acbplmv.json', 'aceolgzx.json', 'ahomx.json', 'aishkqu.json', 'altuxox.json', 'amowrsh.json', 'apakwv.json', 'apmwy.json', 'aqhzllis.json', 'arwvjftm.json', 'asmucefp.json', 'audtaxh.json', 'awlal.json', 'axphpch.json', 'ayteap.json', 'azndzp.json', 'bdwiu.json', 'beoczxhp.json', 'bghlh.json', 'bgxblaq.json', 'bhlwohi.json', 'biaio.json', 'bjwiuevz.json', 'bjzfnftw.json', 'bmmzflr.json', 'bodmvxdf.json', 'bpblgwar.json', 'bphhvc.json', 'bpjzrlxy.json', 'bqbvlmis.json', 'brmtqcj.json', 'brorn.json', 'brwup.json', 'brzpgboe.json', 'bsjgc.json', 'bslps.json', 'btrlegkg.json', 'bttjq.json', 'bvabh.json', 'bvnniatg.json', 'bzavavce.json', 'caaelf.json', 'caeck.json', 'caogdi.json', 'cbuuznl.json', 'ccbje.json', 'cdhwa.json', 'cdruyo.json', 'cehogf.json', 'cfsvmoen.json', 'cftid.json', 'cfwzqt.json', 'chwusgl.json', 'cifmtm.json', 'cifyk.json', 'cioumfyi.json', 'ciuipt.json', 'ciwmb.json', 'cketrvj.json', 'cnhjk.json', 'cnlce.json', 'cqksepwt.json', 'cqqqjx.json', 'cqyvoud.json', 'cqzxj.json', 'crusceee.json', 'ctfljg.json', 'czwglks.json', 'dcidctu.json', 'ddlpfjed.json', 'ddwoml.json', 'debuzvc.json', 'deopslyp.json', 'dgppgjr.json', 'dhrezf.json', 'digdken.json', 'dkgpnac.json', 'doihof.json', 'dqhfisnj.json', 'dqxgzzq.json', 'dsnfmpl.json', 'dsvfsknw.json', 'dupnrc.json', 'dykbpos.json', 'dyzbzmd.json', 'eapzf.json', 'ecpfwumt.json', 'eechlt.json', 'eenhsx.json', 'eeqnxdb.json', 'efkum.json', 'ekauzlfp.json', 'elbnnrs.json', 'elgcpgx.json', 'emtufqje.json', 'epftt.json', 'eplubxxs.json', 'eporowdg.json', 'espxbh.json', 'estbkk.json', 'esxrrht.json', 'eudatvy.json', 'ewsktq.json', 'ewwcso.json', 'eyjdhqq.json', 'eysee.json', 'fajpkgj.json', 'fcpcxixw.json', 'fczhavit.json', 'fdxcbwrj.json', 'ffcrqo.json', 'fgnuh.json', 'fhszf.json', 'fkvuit.json', 'flgtpxn.json', 'fndgjmzm.json', 'fuvxncl.json', 'fuxozggi.json', 'fzgowhq.json', 'gbumd.json', 'gciirlh.json', 'gduwb.json', 'gdxmso.json', 'gedcirqa.json', 'gevdzsvt.json', 'gfvefxbu.json', 'gfwdbf.json', 'gfyrwma.json', 'ggozukjr.json', 'gieoqvia.json', 'gjtxqp.json', 'gjuhqzdv.json', 'gkqtn.json', 'gnothl.json', 'gnzbtght.json', 'goxgfr.json', 'grddrzm.json', 'gtdctoah.json', 'gtxqws.json', 'gurvox.json', 'gwytgt.json', 'gxabdahe.json', 'gxdcpwo.json', 'haifof.json', 'heaeeg.json', 'heiozqq.json', 'hezwgs.json', 'hhabfel.json', 'hltxkki.json', 'hmavwgj.json', 'hmdjklt.json', 'hoqua.json', 'hovra.json', 'hpsaouip.json', 'hpzoxitc.json', 'hskgpx.json', 'hszvzt.json', 'htbdkait.json', 'huumx.json', 'hvmsfmtn.json', 'hxkeks.json', 'hyutx.json', 'ibnqs.json', 'igswa.json', 'ihrpmc.json', 'ihsbfi.json', 'ijkhegq.json', 'ijoyru.json', 'ijsqaud.json', 'imltuesr.json', 'iqeigph.json', 'irhvprt.json', 'irrdkkbn.json', 'itozobft.json', 'ixfqx.json', 'iyfsi.json', 'iypszquv.json', 'jcmkz.json', 'jdscdzrq.json', 'jhptp.json', 'jjomjmu.json', 'jlrqqriz.json', 'jmbmykpe.json', 'jnlam.json', 'jnvplm.json', 'jptfm.json', 'jqbuixz.json', 'jqfuqu.json', 'jspgm.json', 'jsyjyssc.json', 'jubzucis.json', 'juealsvj.json', 'jwfzxxg.json', 'jxiite.json', 'jxjpziw.json', 'jxqpwlw.json', 'jyeyszb.json', 'kbjsadwd.json', 'kbsxru.json', 'kbwcx.json', 'kcjaz.json', 'kcogomz.json', 'kdscbeoa.json', 'kegru.json', 'kfbgivtc.json', 'kgtrkmsz.json', 'khcaied.json', 'klpyg.json', 'kmrpuzwi.json', 'knvdg.json', 'kqghrpia.json', 'kqqjul.json', 'krdwlm.json', 'krjly.json', 'kspcm.json', 'ktjhyivf.json', 'ktmvcnv.json', 'kucnbni.json', 'kxwsy.json', 'kyughy.json', 'kzgrf.json', 'kzhkaelo.json', 'kzvrrq.json', 'lafkhdc.json', 'lalle.json', 'lbokfzil.json', 'lcuod.json', 'lftxjvs.json', 'lhmqz.json', 'lhyku.json', 'livhw.json', 'livtdd.json', 'ljqcj.json', 'ljylnd.json', 'lndjiibk.json', 'lnstrg.json', 'lomurthi.json', 'lpqqysth.json', 'lpzykg.json', 'lsfqtx.json', 'ltxmyss.json', 'lwvwd.json', 'lwzgs.json', 'lxgov.json', 'lyjtcwqm.json', 'lzooyy.json', 'mbfpm.json', 'metwifo.json', 'mgapfvg.json', 'mgvpscd.json', 'mlylezz.json', 'mmigvvwa.json', 'mmizx.json', 'mqncr.json', 'msvrcfr.json', 'mvhtl.json', 'mvjyf.json', 'mvxmadyv.json', 'mvzdvi.json', 'mzguliw.json', 'nfntx.json', 'ngiwn.json', 'nhumd.json', 'niiskl.json', 'niizdbw.json', 'nizpnfpt.json', 'nksrwz.json', 'nkury.json', 'nkxxa.json', 'nlxxo.json', 'nlzggd.json', 'nnccrkk.json', 'nqhxy.json', 'ntrfeys.json', 'nugwhj.json', 'nutlkn.json', 'nwqios.json', 'oatzro.json', 'obkxqf.json', 'ocpni.json', 'odngwf.json', 'ofkcidqa.json', 'ogkbg.json', 'ondlxia.json', 'ootxjgh.json', 'oruixd.json', 'otbmgnv.json', 'oumtq.json', 'ouyuwu.json', 'ovzunke.json', 'oxaepx.json', 'oyuuo.json', 'pbawnp.json', 'pbgmp.json', 'pbkzt.json', 'pcjsleru.json', 'pdkbg.json', 'pdqqm.json', 'pdwmhrga.json', 'pejbfu.json', 'pfhqfrd.json', 'pgtrzghe.json', 'phdommor.json', 'phidq.json', 'pisti.json', 'pkxxb.json', 'plkwghja.json', 'plrzpl.json', 'pohzzvig.json', 'ponpbpet.json', 'psenoob.json', 'psvry.json', 'purnf.json', 'pxkrm.json', 'pyywwcr.json', 'pznqy.json', 'qajnzy.json', 'qbhrsv.json', 'qcaet.json', 'qcaljay.json', 'qetmh.json', 'qfphl.json', 'qgzfwk.json', 'qjpdfr.json', 'qmbrbf.json', 'qnsjufb.json', 'qobxesvq.json', 'qqfdj.json', 'qraso.json', 'qraxhjys.json', 'qrvoln.json', 'qtishmb.json', 'qvkigj.json', 'qvxwlws.json', 'qxxqr.json', 'qzajgt.json', 'qzpupxt.json', 'rcyogl.json', 'rdovq.json', 'rdqpjqlx.json', 'reraeopq.json', 'reukyfwa.json', 'rjbqqdw.json', 'rjukv.json', 'rlnave.json', 'rlyhuzi.json', 'rmxyhbk.json', 'rnhzudzg.json', 'rosxu.json', 'rowjm.json', 'rpukwtef.json', 'rpwonk.json', 'rseuer.json', 'rszjd.json', 'rtdgpvlc.json', 'rtdiyzd.json', 'rteuy.json', 'rthigud.json', 'rvgdymog.json', 'rwmiz.json', 'rwrko.json', 'ryqcne.json', 'ryvjcfnj.json', 'saqelsg.json', 'sbswsnik.json', 'sbzthj.json', 'scbezdx.json', 'sdyiaw.json', 'sffqp.json', 'sgrcqh.json', 'shabqkpo.json', 'shlql.json', 'shorc.json', 'shvuuls.json', 'sivqknlq.json', 'skcbez.json', 'smkcife.json', 'smtqfu.json', 'snldrfht.json', 'sokrdvej.json', 'sorah.json', 'stfrgd.json', 'sxdmgx.json', 'syzkp.json', 'szclqg.json', 'szvzt.json', 'takejb.json', 'tbcbekmq.json', 'tclbku.json', 'tclywy.json', 'tcpjsr.json', 'tfggglf.json', 'tgiyn.json', 'thjpmdp.json', 'tnjlwzwb.json', 'tomluz.json', 'tooal.json', 'tpedb.json', 'tryacy.json', 'tsceov.json', 'tspdree.json', 'tssjsw.json', 'ttain.json', 'ttlztms.json', 'ttqpaslw.json', 'tvkyfq.json', 'tzabfv.json', 'tzkasqnn.json', 'tznjyo.json', 'tzznkxf.json', 'uaalnyoj.json', 'uazvhld.json', 'udghn.json', 'ueaelqhp.json', 'uecth.json', 'ughlzdd.json', 'uhbbh.json', 'ujnnvh.json', 'uktep.json', 'umfmofv.json', 'umqzp.json', 'unbawwp.json', 'uncjwyyp.json', 'unmwiet.json', 'uphoqmv.json', 'urgmrkp.json', 'urjbt.json', 'urwojpfi.json', 'utcniycx.json', 'utste.json', 'uuouaarr.json', 'uvqtpm.json', 'uvtdgfti.json', 'uxdouy.json', 'uypqg.json', 'vamvo.json', 'vbefiwn.json', 'vceoh.json', 'vdxpzkfl.json', 'vgyes.json', 'viduze.json', 'vircjd.json', 'vivfatjo.json', 'vjaghhtu.json', 'vkpfv.json', 'vmaagp.json', 'vougrt.json', 'vpasle.json', 'vptlgja.json', 'vpxuxe.json', 'vrnec.json', 'vteob.json', 'vukas.json', 'vukoi.json', 'vukvkftf.json', 'vvpwfqpw.json', 'vwpazsc.json', 'wagnh.json', 'wbiyyafc.json', 'wbxnhayf.json', 'wcwtzpny.json', 'wdvxr.json', 'werhnlmo.json', 'weuyrc.json', 'weyaq.json', 'wfdvqju.json', 'winew.json', 'wjflwg.json', 'wkabbe.json', 'wkkwq.json', 'wlgmcu.json', 'wmwvatnb.json', 'wnkmdk.json', 'wpzfjnwb.json', 'wqnpthh.json', 'wuafx.json', 'wuptba.json', 'wuuif.json', 'wwisnmd.json', 'wxbhke.json', 'xbczrmz.json', 'xbywiofb.json', 'xdwwn.json', 'xdyech.json', 'xefhoj.json', 'xehokcn.json', 'xekcw.json', 'xelszv.json', 'xfaiss.json', 'xggyx.json', 'xgtpxl.json', 'xhekwdq.json', 'xhfqxyg.json', 'xhokpp.json', 'xicjxs.json', 'xigmw.json', 'xlhpgzu.json', 'xmmhnp.json', 'xmpmap.json', 'xmrhqtqb.json', 'xmuzdze.json', 'xmvvel.json', 'xquntnm.json', 'xtjou.json', 'xtlfyrqe.json', 'xtnlnc.json', 'xvlikyt.json', 'xvzmwpsh.json', 'xwdvabs.json', 'xwgndfxh.json', 'ybbbche.json', 'ybmlhz.json', 'ybppz.json', 'ycvkbx.json', 'ydbfhql.json', 'ydkxsvar.json', 'yfowe.json', 'yftemp.json', 'ygjolah.json', 'ygqsjtxz.json', 'ygrgy.json', 'ygycaic.json', 'yhcpzrfo.json', 'ylvqmlp.json', 'ynikxrtr.json', 'ypmwe.json', 'ypudwfvf.json', 'ypwwzekj.json', 'ytvco.json', 'yujiusv.json', 'yvdelq.json', 'yvxpvlc.json', 'ywosgix.json', 'ywpbst.json', 'yzqamu.json', 'zcjqp.json', 'zfmpy.json', 'zgrzpiay.json', 'zgwwfvlq.json', 'zhncldpt.json', 'zilams.json', 'zkivua.json', 'zkwhp.json', 'zlsqsds.json', 'znnex.json', 'zodhxn.json', 'zoukpqm.json', 'zqaau.json', 'zrnggdny.json', 'zsakwfvj.json', 'zsbkhic.json', 'ztmcj.json', 'zybimtt.json']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Palabras clave a buscar en el campo \"Resultado\"\n",
    "palabras_clave = ['course', 'department', 'faculty', 'other', 'project', 'staff', 'student']\n",
    "\n",
    "# Cargar el JSON desde un archivo\n",
    "with open('datos_limpios.json', 'r', encoding='utf-8') as archivo:\n",
    "    datos = json.load(archivo)\n",
    "\n",
    "# Lista para guardar los títulos con resultado \"NONE\"\n",
    "titulos_con_none = []\n",
    "\n",
    "# Limpiar el campo \"Resultado\" y recoger los títulos requeridos\n",
    "for elemento in datos:\n",
    "    # Encontrar la primera palabra clave en \"Resultado\"\n",
    "    resultado = next((palabra for palabra in palabras_clave if palabra in elemento[\"Resultado\"]), \"NONE\")\n",
    "    # Si el resultado es \"NONE\", añadir el título a la lista\n",
    "    if resultado == \"NONE\":\n",
    "        titulos_con_none.append(elemento[\"Titulo\"])\n",
    "\n",
    "# Opcional: Escribir los títulos con \"NONE\" a un archivo de texto\n",
    "with open('titulos_con_none.txt', 'w', encoding='utf-8') as archivo:\n",
    "    for titulo in titulos_con_none:\n",
    "        archivo.write(titulo + '\\n')\n",
    "\n",
    "# Imprimir la lista de títulos\n",
    "print(titulos_con_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define el directorio base donde están las carpetas de clases\n",
    "base_dir = 'data/splits/train_new/'\n",
    "\n",
    "# El archivo de salida donde se guardará el dataset .jsonl\n",
    "output_file = 'dataset_trainnew.jsonl'\n",
    "\n",
    "# Abrir el archivo de salida en modo de escritura\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Recorrer cada carpeta en el directorio base, donde el nombre de la carpeta es la clase\n",
    "    for class_name in os.listdir(base_dir):\n",
    "        class_dir = os.path.join(base_dir, class_name)\n",
    "        # Asegurarse de que es un directorio\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Recorrer cada archivo JSON en la carpeta de la clase\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                # Asegurarse de que es un archivo\n",
    "                if os.path.isfile(file_path) and file_path.endswith('.json'):\n",
    "                    # Abrir y leer el archivo JSON\n",
    "                    with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                        try:\n",
    "                            content = json.load(json_file)\n",
    "                            # Asegurarse de que 'body' está en el contenido\n",
    "                            if 'Body' in content:\n",
    "                                # Crear la entrada para el archivo .jsonl\n",
    "                                entry = {\"instruction\": f\"Classify the following text in one of this classes, and only reply with said class: course, department, faculty, other, project, staff, student: {content['Body']}\", \"context\": \"You are a classification assistant that takes in texts and classifies them based on their content in one of seven classes: course, department, faculty, other, project, staff, student. Your only reply is the class assigned to the text given.\", \"response\": class_name, \"category\": \"classification\"}\n",
    "                                # Escribir la entrada como una línea JSON en el archivo de salida\n",
    "                                outfile.write(json.dumps(entry) + '\\n')\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Error reading {file_path}. File is not valid JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming: 6623it [00:00, 137984.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed dataset saved to dataset_trainnew_transformed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define la ruta del archivo de entrada y de salida\n",
    "input_file = 'dataset_trainnew.jsonl'\n",
    "output_file = 'dataset_trainnew_transformed.jsonl'\n",
    "\n",
    "# Función para transformar cada entrada del dataset\n",
    "def transform_entry(entry):\n",
    "    # Construir la nueva cadena de texto\n",
    "    transformed_text = f\"<s>[INST] {entry['instruction']} [/INST] {entry['context']} {entry['response']} </s>\"\n",
    "    return {'text': transformed_text}\n",
    "\n",
    "# Leer el archivo de entrada y aplicar la transformación\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Leer el archivo .jsonl línea por línea\n",
    "    for line in tqdm(infile, desc=\"Transforming\"):\n",
    "        # Cargar la entrada JSON\n",
    "        entry = json.loads(line.strip())\n",
    "        # Transformar la entrada\n",
    "        transformed_entry = transform_entry(entry)\n",
    "        # Escribir la entrada transformada en el archivo de salida\n",
    "        outfile.write(json.dumps(transformed_entry) + '\\n')\n",
    "\n",
    "print(f\"Transformed dataset saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

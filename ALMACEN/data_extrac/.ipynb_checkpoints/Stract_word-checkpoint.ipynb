{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788820d5-3ffb-4784-adb3-3d0b9c53bd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-dde4b7b96977>:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dde4b7b96977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Ajustar el pipeline a tus datos.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mX_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# y serían tus etiquetas si estás haciendo aprendizaje supervisado.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Ahora X_features contiene las características extraídas listas para ser usadas en un clasificador.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \"\"\"\n\u001b[1;32m    470\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.11/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1296\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Función para extraer texto del HTML basado en etiquetas específicas.\n",
    "def extract_text_from_html(html, tags=['title', 'meta', 'body']):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        texts = []\n",
    "        for tag in tags:\n",
    "            if tag == 'meta':\n",
    "                for meta in soup.find_all('meta', attrs={\"name\": \"description\"}):\n",
    "                    texts.append(meta.get('content', ''))\n",
    "            else:\n",
    "                element = soup.find(tag)\n",
    "                if element:\n",
    "                    texts.append(element.get_text())\n",
    "        return ' '.join(texts)\n",
    "        \n",
    "folder_path = \"../data/splits/train\"\n",
    "for dirs in os.listdir(folder_path):\n",
    "    # Supongamos que html_contents es una lista de documentos HTML.\n",
    "    html_contents = [os.path.join(folder_path, dirs, filename) for filename in os.listdir(os.path.join(folder_path, dirs))]  # Aquí iría tu lista de HTML.\n",
    "    #print(html_contents)\n",
    "    # Preprocesamiento y vectorización del texto.\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('chi', SelectKBest(chi2, k=1000)),  # k es el número de características que deseas mantener.\n",
    "    ])\n",
    "    \n",
    "    # Extraer texto de cada HTML y almacenarlo en una lista.\n",
    "    text_data = [extract_text_from_html(html) for html in html_contents]\n",
    "    print(text_data)\n",
    "    # Ajustar el pipeline a tus datos.\n",
    "    X_features = pipeline.fit_transform(text_data, y=None)  # y serían tus etiquetas si estás haciendo aprendizaje supervisado.\n",
    "    \n",
    "    # Ahora X_features contiene las características extraídas listas para ser usadas en un clasificador.\n",
    "    print(X_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efd72d-7935-43cc-b627-677d5d6b91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "# Función para obtener la profundidad máxima del árbol DOM\n",
    "def get_dom_tree_depth(element, depth=0):\n",
    "    if hasattr(element, 'children'):\n",
    "        return max((get_dom_tree_depth(child, depth + 1) for child in element.children if child.name is not None), default=depth)\n",
    "    return depth\n",
    "\n",
    "# Función para extraer características de un contenido HTML\n",
    "def extract_html_features(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Obtener el título de la página, si está presente\n",
    "    title = soup.title.string if soup.title else ''\n",
    "    \n",
    "    # Obtener todas las metatags y sus contenidos\n",
    "    meta_tags = {meta.attrs.get('name', ''): meta.attrs.get('content', '') for meta in soup.find_all('meta')}\n",
    "    \n",
    "    # Obtener todos los encabezados y su texto\n",
    "    headers = {f'h{i}': [header.text for header in soup.find_all(f'h{i}')] for i in range(1, 7)}\n",
    "    \n",
    "    # Obtener todos los párrafos y su texto\n",
    "    paragraphs = [p.text for p in soup.find_all('p')]\n",
    "    \n",
    "    # Obtener todos los enlaces y sus hrefs\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Obtener todas las imágenes y sus atributos src y alt\n",
    "    images = [{'src': img['src'], 'alt': img.get('alt', '')} for img in soup.find_all('img')]\n",
    "    \n",
    "    # Obtener todos los formularios\n",
    "    forms = [form for form in soup.find_all('form')]\n",
    "    \n",
    "    # Contar todos los scripts y hojas de estilo\n",
    "    scripts = len(soup.find_all('script'))\n",
    "    stylesheets = len(soup.find_all('link', rel='stylesheet'))\n",
    "    \n",
    "    # Obtener etiquetas semánticas y su contenido\n",
    "    semantic_tags = {tag: [t.text for t in soup.find_all(tag)] for tag in ['article', 'section', 'aside', 'header', 'footer']}\n",
    "    \n",
    "    # Obtener todos los comentarios\n",
    "    comments = [str(comment) for comment in soup.find_all(string=lambda text: isinstance(text, Comment))]\n",
    "    \n",
    "    # Obtener todos los datos estructurados en formato JSON-LD\n",
    "    structured_data = [script.text for script in soup.find_all('script', type='application/ld+json')]\n",
    "    \n",
    "    # Calcular la profundidad del árbol DOM\n",
    "    dom_tree_depth = get_dom_tree_depth(soup)\n",
    "    \n",
    "    # Realizar análisis de frecuencia de palabras clave\n",
    "    text = soup.get_text().lower()\n",
    "    words = text.split()\n",
    "    keyword_frequency = {word: words.count(word) for word in set(words)}\n",
    "    \n",
    "    # Compilar todas las características en un diccionario\n",
    "    features = {\n",
    "        'title': title,\n",
    "        'meta': meta_tags,\n",
    "        'headers': headers,\n",
    "        'paragraphs': paragraphs,\n",
    "        'links': links,\n",
    "        'images': images,\n",
    "        'forms': forms,\n",
    "        'scripts': scripts,\n",
    "        'stylesheets': stylesheets,\n",
    "        'semantic_tags': semantic_tags,\n",
    "        'comments': comments,\n",
    "        'structured_data': structured_data,\n",
    "        'dom_tree_depth': dom_tree_depth,\n",
    "        'keyword_frequency': keyword_frequency\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Ejemplo de cómo usar la función con contenido HTML\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Test HTML</title>\n",
    "    <meta name=\"description\" content=\"This is a test HTML file\">\n",
    "    <meta name=\"keywords\" content=\"test, HTML, file\">\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Main Heading</h1>\n",
    "    <p>This is a paragraph in the body.</p>\n",
    "    <a href=\"http://example.com\">Example link</a>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Extraer características del contenido HTML de ejemplo\n",
    "features = extract_html_features(html_content)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc9f96a-6337-4d87-ad74-769d47c3f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfbaf73b-02a6-4e54-a103-279e83b6b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's write a Python script that uses BeautifulSoup, a popular web scraping library,\n",
    "# to extract the features mentioned from an HTML page.\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "def get_dom_tree_depth(element, depth=0):\n",
    "    if hasattr(element, 'children'):\n",
    "        return max((get_dom_tree_depth(child, depth + 1) for child in element.children if child.name is not None), default=depth)\n",
    "    return depth\n",
    "def get_sumary(texts,sentences_count=5):\n",
    "    # Extract the text from the page's body\n",
    "    page_text = ' '.join(texts)\n",
    "    # Perform summarization using sumy\n",
    "    parser = PlaintextParser.from_string(page_text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)  # You can adjust the number of sentences in the summary.\n",
    "    #print(summary)\n",
    "    return ' '.join([str(sentence) for sentence in summary])\n",
    "# Calculate the DOM tree depth (recursively)\n",
    "def get_dom_tree_depth(element, depth=0):\n",
    "        return max([get_dom_tree_depth(child, depth + 1) for child in element.children if child.name is not None], default=depth)\n",
    "# This function will take HTML content as an input and return extracted features.\n",
    "def extract_html_features(html_content,label='unknow'):\n",
    "    modelo = KeyBERT()\n",
    "    with open(html_content, 'r', encoding='latin-1') as file:\n",
    "        text = file.read()\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    max_link=3\n",
    "    # Initialize a dictionary to store our features\n",
    "    features = {\n",
    "        'title': soup.title.string if soup.title else '',\n",
    "        'summary': get_sumary(soup.stripped_strings,sentences_count=2),\n",
    "        'meta': {meta.attrs['name']: meta.attrs.get('content', '') for meta in soup.find_all('meta', attrs={'name': True})},\n",
    "        'headers': {f'h{i}': [header.text for header in soup.find_all(f'h{i}')] for i in range(1, 3)},\n",
    "        'paragraphs': [p.text for p in soup.find_all('p')],\n",
    "        'links': [a['href'] for a in soup.find_all('a', href=True)][:max_link],\n",
    "        'lists': {\n",
    "            'unordered': get_sumary([ul.text for ul in soup.find_all('ul')]),\n",
    "            'ordered': [ol.text for ol in soup.find_all('ol')]\n",
    "        },\n",
    "        'images': [{'src': img.get('src', ''), 'alt': img.get('alt', '')} for img in soup.find_all('img')],\n",
    "        'forms':  [{'action': form.get('action'), 'method': form.get('method')} for form in soup.find_all('form')],\n",
    "        'scripts': len(soup.find_all('script')),\n",
    "        'stylesheets': len(soup.find_all('link', rel='stylesheet')),\n",
    "        'semantic_tags': {tag: [str(t.text) for t in soup.find_all(tag)] for tag in ['article', 'section', 'aside', 'header', 'footer']},\n",
    "        'comments': len([comment for comment in soup.find_all(string=lambda text: isinstance(text, Comment))]),\n",
    "        'url_structure': '',  # Placeholder for URL, should be filled with the actual URL structure\n",
    "        'structured_data': [script.text for script in soup.find_all('script', type='application/ld+json')],\n",
    "        'dom_tree_depth': '',  # Placeholder for DOM tree depth calculation\n",
    "        'keyword_frequency': {},  # Placeholder for keyword frequency analysis\n",
    "        'keyword_frequency_kebert': {},  # Placeholder for keyword frequency analysis\n",
    "        'ground_truth':label\n",
    "    }\n",
    "    \n",
    "    features['dom_tree_depth'] = get_dom_tree_depth(soup)\n",
    "\n",
    "    # Perform keyword frequency analysis\n",
    "    text = soup.get_text().lower()\n",
    "    words = text.split()\n",
    "    # Lista de stop words en inglés (puedes extenderla o modificarla según sea necesario)\n",
    "    stop_words = set(['the', 'and', 'is', 'in', 'to', 'a', 'of', 'for', 'on', 'that', 'with', 'as', 'it', 'at', 'by', 'from', 'this', 'an', 'be', 'are', 'was', 'or', 'which', 'has', 'you', 'your', 'have', 'but', 'not', 'we', 'they', 'will', 'its', 'their', 'all', 'can', 'more', 'any', 'if', 'my', 'our', 'about', 'may', 'such', 'other', 'into', 'than', 'these', 'her', 'would', 'him', 'his', 'some', 'what', 'over', 'also', 'when', 'out', 'there', 'after', 'first', 'up', 'one', 'last', 'new', 'no', 'most', 'been', 'who', 'its'])\n",
    "    \n",
    "    # Calcular la frecuencia de palabras sin contar las stop words\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    keyword_frequency = {word: words.count(word) for word in set(words)}\n",
    "    palabras_clave = modelo.extract_keywords(text, stop_words='english')\n",
    "    features['keyword_frequency_kebert'] = [palabra for palabra,_ in palabras_clave]\n",
    "    N=10\n",
    "    features['keyword_frequency'] = list(dict(sorted(keyword_frequency.items(), key=lambda item: item[1], reverse=True)[:N]).keys())\n",
    "\n",
    "    return features\n",
    "\n",
    "# Example usage (assuming 'html_content' is a variable containing the HTML source code):\n",
    "# html_features = extract_html_features(html_content)\n",
    "# print(html_features)\n",
    "\n",
    "# Since we cannot scrape a live webpage, we would need HTML content provided to utilize this function.\n",
    "# If you have an HTML file or string you would like to analyze, please provide it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2098b591-a84e-4947-b1fb-6854f0815b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'ECE/CS 752 Spring 1996',\n",
       " 'summary': 'Date: Mon, 11 Nov 1996 16:02:43 GMT Server: NCSA/1.5.2 Last-modified: Fri, 10 May 1996 15:09:04 GMT Content-type: text/html Content-length: 7346 ECE/CS 752 Spring 1996 ECE/CS 752: Advanced Computer Architecture I Spring 1996 Offering Course Information Instructor: Prof. James E. Smith Office:         4621 Engineering Hall Office hours:   2:30-3:30PM, Tues. and Thurs. Office phone:   265-5737 Email: jes@ece.wisc.edu TA: Dan Prince Office:         3614 Engineering Hall Office hours:   2:00-3:00PM Wed; 1:00-2:00PM Fri Office phone:   265-3825 E-mail address: princed@cae.wisc.edu Table of Contents News Readings Lecture Notes Homeworks Project Miscellaneous News Homework 5 solns  5/10 Special Office hours: 10-11:30AM Fri. May 10 FINAL EXAM: Rm 132 Biochemistry, Sun.',\n",
       " 'meta': {},\n",
       " 'headers': {'h1': ['ECE/CS 752: Advanced Computer Architecture I ',\n",
       "   'Spring 1996 Offering'],\n",
       "  'h2': [' Table of Contents',\n",
       "   'News',\n",
       "   'Readings',\n",
       "   ' Lecture Notes',\n",
       "   'Homeworks',\n",
       "   'Project',\n",
       "   'Miscellaneous']},\n",
       " 'paragraphs': [],\n",
       " 'links': ['http://www.ece.wisc.edu/~jes/752/conduct.ps',\n",
       "  'http://www.engr.wisc.edu/ece/faculty/smith_james.html',\n",
       "  'http://www.engr.wisc.edu/cgi-bin/mailto-form?jes@ece.wisc.edu'],\n",
       " 'lists': {'unordered': 'Special Office hours: 10-11:30AM Fri. May 10 Advanced Pipelining, part 1 Advanced Pipelining, part 2 I/O and disk arrays (BIG) Midterm Exam from 1995',\n",
       "  'ordered': []},\n",
       " 'images': [],\n",
       " 'forms': [],\n",
       " 'scripts': 0,\n",
       " 'stylesheets': 0,\n",
       " 'semantic_tags': {'article': [],\n",
       "  'section': [],\n",
       "  'aside': [],\n",
       "  'header': [],\n",
       "  'footer': []},\n",
       " 'comments': 189,\n",
       " 'url_structure': '',\n",
       " 'structured_data': [],\n",
       " 'dom_tree_depth': 28,\n",
       " 'keyword_frequency': ['homework',\n",
       "  '(big)',\n",
       "  '1',\n",
       "  'part',\n",
       "  '2',\n",
       "  'assignment',\n",
       "  'readings',\n",
       "  'office',\n",
       "  'solution',\n",
       "  'advanced'],\n",
       " 'keyword_frequency_kebert': ['overview',\n",
       "  'biochemistry',\n",
       "  'architecture',\n",
       "  'engineering',\n",
       "  'edu'],\n",
       " 'ground_truth': 'unknow'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_html_features('../data/splits/train/course/achmly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "111e6ee7-e195-4209-b693-26c0d3d54677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo datos: 100%|███████████████████████| 109/109 [00:23<00:00,  4.63it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 403/403 [01:26<00:00,  4.65it/s]\n",
      "Extrayendo datos:  10%|██▎                   | 309/3011 [01:16<09:05,  4.95it/s]<ipython-input-55-dc2b7d3831da>:32: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n",
      "Extrayendo datos: 100%|█████████████████████| 3011/3011 [12:19<00:00,  4.07it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 899/899 [03:14<00:00,  4.61it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 145/145 [00:30<00:00,  4.83it/s]\n",
      "Extrayendo datos: 100%|█████████████████████| 1312/1312 [04:39<00:00,  4.69it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 744/744 [02:51<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import json\n",
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"../data/splits/train\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"../data/splits/train_new_data/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Process all files in the folder\n",
    "for dirs in os.listdir(folder_path):\n",
    "    for filename in tqdm.tqdm(os.listdir(os.path.join(folder_path, dirs)), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, dirs, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_html_features(file_path,dirs)\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder,dirs)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa82b365-d41f-4cf7-93f8-b90cc2a47bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json\n",
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"../data/splits/test\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"../data/splits/test_new_data/test\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Process all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_html_features(file_path)\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder,dirs)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb52172-c85f-403f-9933-60919bf1c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"../data/splits/train\"\n",
    "for dirs in os.listdir(folder_path):\n",
    "    # Supongamos que html_contents es una lista de documentos HTML.\n",
    "    html_contents = [os.path.join(folder_path, dirs, filename) for filename in os.listdir(os.path.join(folder_path, dirs))]  # Aquí iría tu lista de HTML.\n",
    "    #print(html_contents)\n",
    "    # Preprocesamiento y vectorización del texto.\n",
    "    \n",
    "    # Extraer texto de cada HTML y almacenarlo en una lista.\n",
    "    text_data = [extract_html_features(html) for html in html_contents]\n",
    "    print(text_data)\n",
    "    # Ajustar el pipeline a tus datos.\n",
    "    # Ahora X_features contiene las características extraídas listas para ser usadas en un clasificador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e8e93-17dc-41fe-a81a-2e0007ac447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_links(soup):\n",
    "    \"\"\" Extrae todos los enlaces y sus textos de un objeto BeautifulSoup. \"\"\"\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        url = a_tag['href']\n",
    "        text = a_tag.get_text(strip=True)\n",
    "        links.append((url, text))\n",
    "    return links\n",
    "\n",
    "def get_most_representative_links(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = extract_links(soup)\n",
    "\n",
    "    # Contar frecuencia de cada enlace\n",
    "    link_freq = Counter(links)\n",
    "\n",
    "    # Filtrar por enlaces con texto descriptivo\n",
    "    descriptive_links = [(url, text) for url, text in links if len(text) > 3]\n",
    "\n",
    "    # Contar la frecuencia de los enlaces descriptivos\n",
    "    descriptive_link_freq = Counter(descriptive_links)\n",
    "\n",
    "    # Podrías ordenar por frecuencia y tomar los N enlaces más frecuentes o descriptivos\n",
    "    N = 20\n",
    "    most_common_links = descriptive_link_freq.most_common(N)\n",
    "\n",
    "    return most_common_links\n",
    "\n",
    "folder_path = \"../data/splits/train/course\"\n",
    "\n",
    "for dirs in os.listdir(folder_path):\n",
    "    #extract_html_features()\n",
    "    with open(os.path.join(folder_path, dirs), 'r', encoding='latin-1') as file:\n",
    "            html_content = file.read()\n",
    "    # Usar la función con tu contenido HTML\n",
    "    most_representative_links = get_most_representative_links(html_content)\n",
    "    print(most_representative_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed274a0f-584a-44a4-adbe-87ac07066813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def extract_links_html(soup):\n",
    "    \"\"\" Extrae todos los enlaces y sus textos de un objeto BeautifulSoup. \"\"\"\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        url = a_tag['href']\n",
    "        text = a_tag.get_text(strip=True)\n",
    "        links.append((url, text))\n",
    "    return links\n",
    "\n",
    "def extract_links(html_content):\n",
    "    \"\"\" Extrae todos los enlaces (href) de un objeto BeautifulSoup. \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return extract_links_html(soup)\n",
    "\n",
    "def find_common_urls(html_documents):\n",
    "    \"\"\" Encuentra los URLs más comunes en una lista de documentos HTML. \"\"\"\n",
    "    all_links = []\n",
    "    \n",
    "    # Extraer enlaces de cada documento HTML\n",
    "    for html_content in html_documents:\n",
    "        links = extract_links(html_content)\n",
    "        all_links.extend(links)\n",
    "    \n",
    "    # Contar la frecuencia de cada enlace\n",
    "    link_counter = Counter(all_links)\n",
    "    \n",
    "    # Obtener los enlaces más comunes\n",
    "    most_common_links = link_counter.most_common()\n",
    "    \n",
    "    return most_common_links\n",
    "\n",
    "\n",
    "folder_path = \"../data/splits/train/course\"\n",
    "# Suponiendo que tienes una lista de contenido HTML en html_contents\n",
    "# Aquí tendrías que cargar tus documentos HTML en la lista html_contents\n",
    "html_contents = []\n",
    "# Listar todos los archivos en el directorio\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Construir la ruta completa al archivo\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    \n",
    "        # Abrir y leer el contenido del archivo\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            html_content = file.read()\n",
    "            html_contents.append(html_content)\n",
    "            \n",
    "print(html_contents)\n",
    "# Encontrar y mostrar los URLs más comunes\n",
    "common_urls = find_common_urls(html_contents)\n",
    "for url, count in common_urls:\n",
    "    print(f\"URL: {url}, Frecuencia: {count} len: {len(html_contents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785e42f-4078-4a6f-bb9c-5af939862c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar el conjunto de stop words si aún no se ha hecho (se hace una sola vez)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar las stop words en inglés\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_html(html_content):\n",
    "    \"\"\" Elimina todas las etiquetas HTML y devuelve solo el texto. \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "    return text\n",
    "\n",
    "def get_most_representative_words(html_documents):\n",
    "    \"\"\" Encuentra las palabras más representativas en una lista de documentos HTML. \"\"\"\n",
    "    all_words = []\n",
    "\n",
    "    # Procesar cada documento HTML\n",
    "    for html_content in html_documents:\n",
    "        text = clean_html(html_content)\n",
    "        # Convertir a minúsculas y tokenizar\n",
    "        words = word_tokenize(text.lower())\n",
    "        # Eliminar stop words y palabras que no sean alfabéticas\n",
    "        words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    # Contar la frecuencia de cada palabra\n",
    "    word_counter = Counter(all_words)\n",
    "    \n",
    "    # Obtener las palabras más comunes\n",
    "    most_common_words = word_counter.most_common()\n",
    "    \n",
    "    return most_common_words\n",
    "\n",
    "# Supongamos que ya tienes una lista de contenido HTML llamada html_contents\n",
    "# Aquí iría el código para cargar tus documentos HTML en la lista html_contents\n",
    "\n",
    "# Encontrar y mostrar las palabras más representativas\n",
    "representative_words = get_most_representative_words(html_contents)\n",
    "for word, count in representative_words:\n",
    "    print(f\"Palabra: {word}, Frecuencia: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8f10c-acb4-428d-8767-bbe89953007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = KeyBERT()\n",
    "modelo.extract_keywords(html_contents, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e97ae-ac6d-416c-81c1-61b0b108839a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a8df31-3830-47d3-a5c3-a6acdc2a0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (3.8.1)\n",
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from sumy) (2.31.0)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting chardet (from breadability>=0.1.20->sumy)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
      "  Downloading lxml-4.9.3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: setuptools in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from pycountry>=18.2.23->sumy) (68.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alourido/anaconda3/envs/alpaca/lib/python3.11/site-packages (from requests>=2.7.0->sumy) (2023.7.22)\n",
      "Downloading lxml-4.9.3-cp311-cp311-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=ec8a026e52141a5f69e8adc17dce46626b4af8ebcd95fa0d6e0cfcb10bcdc6f1\n",
      "  Stored in directory: /home/alourido/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=c913125f8d23c52c1a7c5995e0adb86feff94f87b0174c179368dc50840908a8\n",
      "  Stored in directory: /home/alourido/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=aa15f3fd379717de9ca8a88b6761c56bf234a3ca2fc6f6100ae881264ff39e61\n",
      "  Stored in directory: /home/alourido/.cache/pip/wheels/cd/29/8b/617685ed7942656b36efb06ff9247dbe832e3f4f7724fffc09\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, lxml, chardet, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 lxml-4.9.3 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4b340f-e6d1-4e7a-a403-d0e4cb91d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alourido/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "import nltk\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "nltk.download('punkt')\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898f6776-f9ed-47a1-949b-54e07ab1a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo datos: 100%|██████████████████████| 109/109 [00:00<00:00, 194.03it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 403/403 [00:03<00:00, 119.41it/s]\n",
      "Extrayendo datos:  10%|██▏                   | 302/3011 [00:10<01:08, 39.65it/s]<ipython-input-2-37577477ebb1>:17: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, 'html.parser')\n",
      "Extrayendo datos: 100%|█████████████████████| 3011/3011 [01:40<00:00, 29.90it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 899/899 [00:07<00:00, 114.31it/s]\n",
      "Extrayendo datos: 100%|██████████████████████| 145/145 [00:00<00:00, 222.24it/s]\n",
      "Extrayendo datos: 100%|████████████████████| 1312/1312 [00:06<00:00, 196.91it/s]\n",
      "Extrayendo datos: 100%|███████████████████████| 744/744 [00:09<00:00, 80.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function defined to extract the title and summary from the page.\n",
    "def extract_title_and_summary(file_path,dirs, limit_link=3,sentences_count=1):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Analyze the HTML content using BeautifulSoup.\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract the page title\n",
    "    title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all('a'):\n",
    "        if 'href' in a.attrs:\n",
    "            if len(links) < limit_link:\n",
    "                links.append(a['href'])\n",
    "            else:\n",
    "                break\n",
    "        #print(\"Enlaces en la página:\", links)\n",
    "\n",
    "    # Extract the text from the page's body\n",
    "    page_text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "    # Perform summarization using sumy\n",
    "    parser = PlaintextParser.from_string(page_text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)  # You can adjust the number of sentences in the summary.\n",
    "    #print(summary)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "\n",
    "    return {\"Title\": title, \"Body\": summary_text, \"link\": links ,\"ground_truth\": dirs}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22712423-e9cf-4605-98d1-c1cd5ed7c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"../data/splits/train\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"../data/splits/train_new_short/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Process all files in the folder\n",
    "for dirs in os.listdir(folder_path):\n",
    "    for filename in tqdm.tqdm(os.listdir(os.path.join(folder_path, dirs)), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, dirs, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_title_and_summary(file_path,dirs)\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder,dirs)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e379e012-4bf2-4e82-b159-8d14b126aa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo datos: 100%|█████████████████████| 1659/1659 [01:11<00:00, 23.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"../data/splits/test\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"../data/splits/test_new_shor/test\"\n",
    "# Process all files in the folder\n",
    "for filename in tqdm.tqdm(os.listdir(folder_path), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_title_and_summary(file_path,'unknow')\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af9694-fe40-48fd-8512-a1b60916f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_and_summary_2(file_path,dirs, limit_link=3,sentences_count=1):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Analyze the HTML content using BeautifulSoup.\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract the page title\n",
    "    title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all('a'):\n",
    "        if 'href' in a.attrs:\n",
    "            if len(links) < limit_link:\n",
    "                links.append(a['href'])\n",
    "            else:\n",
    "                break\n",
    "        #print(\"Enlaces en la página:\", links)\n",
    "\n",
    "    # Extract the text from the page's body\n",
    "    page_text = ' '.join(soup.stripped_strings)\n",
    "\n",
    "    # Perform summarization using sumy\n",
    "    parser = PlaintextParser.from_string(page_text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=sentences_count)  # You can adjust the number of sentences in the summary.\n",
    "    #print(summary)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "\n",
    "    return {\"Title\": title, \"Body\": summary_text, \"link\": links ,\"ground_truth\": dirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2241f-b9ce-479d-8266-4b22aa142396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path containing the original files\n",
    "#folder_path = \"C:/Users/user/Downloads/dataset_splitted/splits/test\"\n",
    "folder_path = \"../data/splits/train\"\n",
    "\n",
    "# Output folder path for JSON files\n",
    "#output_folder = \"C:/Users/user/Downloads/dataset_splitted/splits/test_json\"\n",
    "output_folder = \"../data/splits/train_new_short/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Process all files in the folder\n",
    "for dirs in os.listdir(folder_path):\n",
    "    for filename in tqdm.tqdm(os.listdir(os.path.join(folder_path, dirs)), desc=\"Extrayendo datos\"):\n",
    "        file_path = os.path.join(folder_path, dirs, filename)\n",
    "        #print(file_path)\n",
    "        if os.path.isfile(file_path):\n",
    "            formatted_data = extract_title_and_summary(file_path,dirs)\n",
    "            #print(formatted_data)\n",
    "            #print(\"Processed:\", filename)\n",
    "            #print(\"Title:\", formatted_data[\"Title\"])\n",
    "            #print(\"Summary:\", formatted_data[\"Body\"])\n",
    "            #print(\"Ground Truth:\", formatted_data[\"ground_truth\"])\n",
    "            # Save the data to a JSON fileç\n",
    "            output_folder_dir=os.path.join(output_folder,dirs)\n",
    "            os.makedirs(output_folder_dir, exist_ok=True)\n",
    "            output_json_file = os.path.join(output_folder_dir, os.path.splitext(filename)[0] + \".json\")\n",
    "            with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(formatted_data, json_file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
    "TÃ­tulo": "What Is Importance-Based Feature Extraction?",
    "Cuerpo": "The main idea Suppose that an autonomous agent has feature detectors which identify its state, and that it uses reinforcement learning to learn to succeed in its environment. Importance-based feature extraction aims to tune the agent's feature detectors to be most sensitive to states where the agent's choice of action is critical. If the agent's action from this state will have little bearing on the agent's future success, we say that the state is unimportant ; we define an important state as one having very different predictions of reinforcement for taking different actions. In terms of Q-learning, this means that an important state is one from which the different actions have very different Q-values. For example, if the agent frequently finds itself in a particular state where almost any action is equally good, frequency-based feature tuning would cluster detectors around that state; however, those detectors will be of little use to the agent because its choice of action at this state has little bearing on its future reinforcement. Or the agent may find itself in a rarely-seen state where its choice of action is critical for future success; such a state is important, though infrequent. Furthermore, important states need not be associated with the most extreme reinforcement values. For example, there may be a state from which the agent will fail, no matter what action it takes. But detecting this state is not very helpful to the agent, because when it is in this state there is nothing it can do to prevent failure. The Q-values at this state might not be as great in absolute magnitude as those associated with a state from which the agent always fails, or a state from which the agent always succeeds. But since some actions from this state lead to success and some to failure, it has a greater span of Q-values associated with the actions; this makes it an important state. When our goals require us to respond to some particular features of an individual, we need to learn to recognize those features which make this individual a special case. But skiers talk about more varieties of snow, and the distinctions are relevant to them because different kinds of snow will have different effects on their skiing. This is an example of importance-based feature extraction, since we are \"tuning\" our \"feature detectors\" to respond to those features which make a difference in the things we have to do, and otherwise falling back on broad stereotypes. Importance-based feature extraction attempts to tune the feature detectors according to their importance in selecting the agent's actions; a detector is considered important if the links from it to the outputs have very different weights. In other words, this detector is valuable because it detects a state from which the agent's choice of action will strongly affect the agent's liklihood of success. What's new about importance-based feature extraction? Kohonen's Self-Organized Map and related clustering methods attempt to distribute the feature detectors according to the probability density function of the states seen by the agent. Chapman & Kaelbling's concept of \"relevance\" biases feature extraction toward the detection of features which are associated with extreme reinforcement values. But I am not aware of a concept like importance-based feature extraction in statistics.",
    "ground_truth": "other"
}
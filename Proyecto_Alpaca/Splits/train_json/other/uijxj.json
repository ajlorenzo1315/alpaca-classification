{
    "TÃ­tulo": "Symbol Emergence & Symbol Grounding",
    "Cuerpo": "On one side is classical AI, maintaining that intelligence is a matter of symbol processing. What has been missing from this dialog is a discussion of how symbols and symbol manipulation come about in the human mind. That is, artificial intelligence is realized by representing a problem as a set of symbols which the computer can then manipulate to find a correct and optimal solution. Now, what about \"it\"? This can be done with scripts that describe these relationships [Schank and Abelson 1977], but now our implementation is quite daunting: In order to understand natural language in an unconstrained setting, we would need to be armed with a battery of scripts to parse some of the simplest texts. The other argument against representational AI is that some knowledge cannot be captured by a set of rules. I think this is closer to the matter, because it concedes that the logical approach might not work all the time. Connectionism, a sister to representation, held a lot of promise and excelled at problems that representation struggled with [Rosenblatt 1960a, 1960b, 1962; Steinbuch 1961; Widrow 1962; Grossberg 1968] (and, predictably, vice versa). (This is similar to Searle's criticism that, just because a computer does brain-stuff, doesn't mean that it has a mind.) For example, suppose you have a robot with two light sensors and two motors. If the meaning of abstract symbols are derived from their correspondence to objects in the real world, then categories must exist in the real world in order for them to be meaningful. But the categories that we use do not follow the rules of logic. What, then, is the origin of symbol manipulation in the brain? Internal Realism assumes that a) there is a real world that we all live in, b) that our concepts of that world are determined by our sensory apparatus, but since c) all people have the same basic physiology then d) our conceptual system is based on a universal foundation of ideas about the world. These are the robot's base-level concepts. What is needed is an internal motivation for the robot, and an ability to tap into these new categories as an aid towards helping it perform its tasks. In P.H. Winston, ed., Artificial Intelligence at MIT , vol. A. \"The hippocampus as a cognitive graph\" New York University, in press.",
    "ground_truth": "other"
}
{
    "TÃ­tulo": "Vision and Touch Guided Manipulation",
    "Cuerpo": "Date: Tue, 26 Nov 1996 00:01:31 GMT Server: Apache/1.2-dev Connection: close Content-Type: text/html Last-Modified: Tue, 03 Sep 1996 16:28:10 GMT ETag: \"5d872-3cd8-322c5c9a\" Content-Length: 15576 Accept-Ranges: bytes Vision and Touch Guided Manipulation Vision and Touch Guided Manipulation Group MIT Artificial Intelligence Lab & Nonlinear Systems Lab The Vision and Touch Guided Manipulation group at the MIT Artificial Intelligence Lab conducts research in a wide variety of topics related to manipulator and end effector design, dextrous manipulation, adaptive nonlinear control, and vision guided manipulation. The people in and associated with the Vision and Touch Guided Manipulation Group are: Brian Anthony ( touch sensing ) Mark Cannon ( wavelet networks ,graduated ) Brian Eberman ( system integration, ,graduated ) Brian Hoffman ( active vision ) W. Jesse Hong ( coordination vision-manipulation ) Akhil Madhani ( wrist-hand mechanism ) G&uumlnter Niemeyer ( adaptive control and system integration ) Daniel Theobald ( visual processing ) Ichiro Watanabe ( machine learning ) [Introduction] [Our Robots] [Our Research] [References] Introduction to our Robots The Whole Arm Manipulator The MIT Whole Arm Manipulator (WAM) Arm is a very fast, force controllable robot arm designed in Dr. Salisbury's group at the AI Lab. Central to this concept (and our group's design efforts in general) has been a focus on controlling the forces of interaction between robots and the environment. To achieve good bandwidth in force control while in contact with the environment, the arm's design maximizes the lowest resonant frequency of the system and employs an impedance matching ratio between motor and arm masses. This also enables the arm to achieve high accelerations while moving in free space. We also have studied the design of a miniature end-effector suitable for grasping small rocks and cylindrical objects. Similar in spirit to the Talon, the new miniature end-effector utilizes slightly different kinematics to enlarge its feasible grasping volume. The independent nature of the FEGs allow us to position each one at different locations in order to vary the baseline or orientation of the coordinate frame as well as easily add additional cameras to provide additional perspectives. We have implemented a high-speed active vision system, a multi-processor operating system, and basic algorithms for acquisition and grasp of stationary spherical and cylindrical objects using coordinated robotic vision, touch sensing, and control. Preliminary experiments on the tracking of moving objects have also been completed. Each camera signal is processed independently on vision boards designed by other members of the MIT AI Laboratory (the Cognachrome Vision Tracking System ). In addition to the basic least squares techniques for path prediction, we study experimentally nonlinear estimation algorithms to give \"long term\" real-time prediction of the path of moving objects, with the goal of robust acquisition. As a initial step, we have studied the network's performance in predicting the path of light objects thrown in air. Slotine, Proceedings of the Fourth International Symposium on Experimental Robotics, ISER'95, Stanford, California, June 30-July 2, 1995. Robotic Catching and Manipulation Using Active Vision , W. Hong, M.S. Thesis, Department of Mechanical Engineering, MIT, September 1995. Slotine, International Journal of Robotics Research 10(2), December, 1988. Preliminary Design of a Whole-Arm Manipulation System (WAM) , J.K. Salisbury, W.T. Townsend, PhD Thesis, Department of Mechanical Engineering, MIT, April 1988. Design and Control of a Two-Axis Gimbal System for Use in Active Vision , N. Swarup, S.B.",
    "ground_truth": "project"
}